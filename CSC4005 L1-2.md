## L1
### Grand challenge problems
* large DNA structures, global weather forcasting, motion of astronomical bodies
### Shared Memory Multiprocessor Systems
#### Parallel programming languages
* with special parallel programming constructs and statements that allow shared variables and parallel code sections to be declared.
#### Threads
* high-level language code sequences for individual processors
### Message Passing Multiprocessor Systems
* messages processor
* local memory
* interconnection network
### SISD (Flynn分类法)
* calles this single processor Single Instruction Stream-Single Data Stream (SISD)
    * 所有指令都是串行执行，CPU只能处理一个数据流

* Multiple Instruction Stream-Multiple Data Stream (MIMD)
* Single Instruction Streaming-Multiple Data Streaming (SIMD)
    * multiple processors. Each processor executes the same instruction **in synchronism**, but using different data

#### Multiple Program Multiple Data Streaming (MPMD) Structure
* each processor will have its own program to execute
![bb043926-f90d-4a75-b6d6-2b72450a433f.jpg](CSC4005 L1-2_files/bb043926-f90d-4a75-b6d6-2b72450a433f.jpg)
### Single Program Multiple Data (SPMD) Structure
* Single source program is written, each processor will execute its personal copy of this program
* **not in synchronism**
### Message-Passing Multicomputers
* static network message-passing multicomputers
![945829ac-4a37-4fcd-adcc-56d874bf0bef.png](CSC4005 L1-2_files/945829ac-4a37-4fcd-adcc-56d874bf0bef.png)
* node with a switch for internode message transfters
![bc22789b-9380-4723-be7b-dd60ffbfb9bb.jpg](CSC4005 L1-2_files/bc22789b-9380-4723-be7b-dd60ffbfb9bb.jpg)
* link between two nodes with separate wires in each direction
![13c9c0b1-c925-4517-9e84-beccf26a68fd.jpg](CSC4005 L1-2_files/13c9c0b1-c925-4517-9e84-beccf26a68fd.jpg)
### Network Criteria
![dca96278-0d24-4564-8eb9-cd968d81e042.jpg](CSC4005 L1-2_files/dca96278-0d24-4564-8eb9-cd968d81e042.jpg)
#### Embdding
* describes mapping nodes of one network onto another network
* dilation - used to indicate the quality of embdding, is the maximum number of links in the "embedding" network corresponding to one link in the "embedded" network
    * 详见sub4.pdf

![6f911235-6f47-4979-ad93-502b2831e679.jpg](CSC4005 L1-2_files/6f911235-6f47-4979-ad93-502b2831e679.jpg)
### Communication Methods
#### Circuit Switching
* establishing path and maintaining all links in path for message to pass. All links are reserved for the transfer until message transfer is complete.
* e.g. simple telephone system: once a telephone connection is made, the connection is maintained until the completion of the telephone call
#### Packing Switching
* Message divided into "packets" of information, each of which includes source and destination addresses for routing packet through interconnection network. 
* If message is larger than maximum size for the packet (e..g 1000 data bytes), more than one packet must be sent through network. 
* Buffers provided inside nodes to hold packets before they are transferred onward to the next node
* *store-and-forward packet switching*
* e.g. mail system. letters moved from mailbox to post office and handled at itermediate sites before delivered to destination
#### Virtual Cut-Through
* eliminated storage latency
* if outgoing link is available, the message is immediately passed forward without being stored in the nodal buffer; if path is blocked, storage is needed for complete message/packet being received.
#### Wormhole Routing
* message divided into smaller units called flits (flow control digits)
* Only head of message initially transmitted from source node to next node when connecting link available. Subsequent flits of message transmitted when links become available.
![c08984be-b33b-4fba-bdab-f86f4f524021.png](CSC4005 L1-2_files/c08984be-b33b-4fba-bdab-f86f4f524021.png)
![512177b2-a7fc-4b2b-836e-ec8c0663f206.jpg](CSC4005 L1-2_files/512177b2-a7fc-4b2b-836e-ec8c0663f206.jpg)
### Deadlock
* Occurs when packets can't be forwarded to next node because they are blocked by other packets waiting to be forwarded and these packets are blocked in a similary way such that none of the packets can move
* e.g.
![edae07de-132e-4416-aa7e-900b048fb81d.jpg](CSC4005 L1-2_files/edae07de-132e-4416-aa7e-900b048fb81d.jpg)
#### Solution 1: Virtual Channel
* Multiple *virtual* channels are associated with a physical channel and time-multiplexed onto the physical channel
![e588b292-a712-42d7-887c-b2dddf9643d2.jpg](CSC4005 L1-2_files/e588b292-a712-42d7-887c-b2dddf9643d2.jpg)
### Network
#### Ring Structures
![d36d9df2-9d76-4a96-84fd-62ed2fa47503.jpg](CSC4005 L1-2_files/d36d9df2-9d76-4a96-84fd-62ed2fa47503.jpg)
#### Point-to-Point Communication
* Provides the highest interconnection bandwidth
![94941ad4-f451-46cf-a009-a0c6b14f0808.jpg](CSC4005 L1-2_files/94941ad4-f451-46cf-a009-a0c6b14f0808.jpg)
#### Overlapping Connectivity Networks
* e.g.
![7b0ce27b-af42-440f-a86f-561b0e63fab7.png](CSC4005 L1-2_files/7b0ce27b-af42-440f-a86f-561b0e63fab7.png)
### Speedup Factor
* Maximum speedup is n with n processors (linear speedup)
![665157db-9211-4d20-ad62-fd9b1a1d05a3.jpg](CSC4005 L1-2_files/665157db-9211-4d20-ad62-fd9b1a1d05a3.jpg)
![24c18542-4b70-493b-af67-3f2beb0e387d.jpg](CSC4005 L1-2_files/24c18542-4b70-493b-af67-3f2beb0e387d.jpg)
#### Amdahl's Law
* $S(n) =$ ${t_s}\over {ft_s + (1-f)t_s/n} $ $=$ $ {n} \over {1+(n-1)f}$
#### Efficiency
* $E = \frac {Execution time using one processor} {Execution time using a multiprocessor \times number of processors} = \frac {t_s} {t_p \times n} = \frac {S(n)} {n} \times 100%$
### Cost
* $Cost = (execution time) \times (total number of processors used) = \frac {t_sn}{S(n)} = \frac {t_s} {E}$
    * $t_s$ is the execution time of a sequential computation

    * $t_p \times n$ is the cost of a parallel computation

#### Scalability
* architecutre scalability: a hardware design that allows the system to be increased in size and in doing so to obtain increased performance
* algorithmic scalability: a parallel algorithm can accommodate increased data items with a low and bounded increase in computational steps
#### Gustafson' Law
* $S_s(n) = \frac {s+np}{s+p} = s+np = n+(1-n)s$
* e.g. serial section of 5% and 20 processors. 0.05+0.95(20) = 19.05


## L2 Message Passing Computing and Programming
### Single Program Multiple Data Model (SPMD)
* Different processes are merged into one program. 
### MPMD Model
* Separate program written for each processors. Master-slave approachusually taken whereby a single processor executes a master process and other processes are started from within the master process
### Synchronous Message Passing
* Routines that actually return when the messages transfer has been completed
* Do not need message buffer storage.
* A synchronous send routine could wait until the complete message can be accepted by the receiving process before sending the message
* A s receive routine will wait until the message it is expectig arrives.
### Blocking and Nonblocking Message Passing
#### Blocking
* has been used to describe routines that do not return until the transfer is completed
* *synchronous* and *blocking* were synonymous
* In MPI, it is: return after their local actions complete, though the message transfer may not have been completed
#### Non-blocking
* routines that return whether or not the message had been received
* In MPI, return immediately. 
* How message-passing routines can return before the message transfer has been completed
    * generally, a message buffer needed between source and destination to hold message

    * for send routine, once the local actions have been completed and the message is safely on its way, the process can continue with subsequent work.

    *    Buffer 长度一定，满了之后，send routine is held up

![4568ca06-9258-4b1b-bd07-d368da5e5208.jpg](CSC4005 L1-2_files/4568ca06-9258-4b1b-bd07-d368da5e5208.jpg)
### Functions
#### Message Tag
* If special type matching is not required, a *wild card* message tag is used, so that the rece() will match with any send()
* e.g. message x with tag 5. The message tag is carried within the message
    * $send(&x,2,5)$

    * $recv(&y,1,5)$

#### Broadcast
* sending the same message to all the processes concerned with the problem
![6ed3bb66-552b-4b60-9021-dea7f8807a7e.jpg](CSC4005 L1-2_files/6ed3bb66-552b-4b60-9021-dea7f8807a7e.jpg)
#### Scatter
* sending each element of an array of data in the root to a separate process. ith location of array is sent to ith process
![52dd1c39-395c-4aa8-a35d-861f1bca1a6e.jpg](CSC4005 L1-2_files/52dd1c39-395c-4aa8-a35d-861f1bca1a6e.jpg)
#### Gather
* having on process collect individual values from a set of processes
![41a76eaf-1a33-4677-8513-01a0d9432725.jpg](CSC4005 L1-2_files/41a76eaf-1a33-4677-8513-01a0d9432725.jpg)
#### Reduce
* gather operation combined with a specified arithmetic or logical operation
### Parallel Virtual Machine (PVM)
* Provides for a software environment for message passing between homogeneous or heterogeneous computes and has a collection of library routines that the user can employ with C or FORTRAN programs
* the set of computers used on a problem first must be defined prior to running the programs
* 最方便的方法是 creating a list of the names of the computers available in a  *hostfile*. The hostfile is then read by PVM
* Routing of message between computers is done by PVM daemon processes installed by PVM on the computers that form the virtual machine
![e97433e5-67ba-4134-826a-e908104b0823.jpg](CSC4005 L1-2_files/e97433e5-67ba-4134-826a-e908104b0823.jpg)
![6093d01a-a853-4ffe-bbbb-3941a9b948e2.jpg](CSC4005 L1-2_files/6093d01a-a853-4ffe-bbbb-3941a9b948e2.jpg)


### Basic Message-Passing Routines
* All PVM sending routines are nonblocking, receive routines can be either blocking or nonblocking
* If data being sent is a list items of the same data type, the PVM routines $pvm_psend()$ and $pvm precv()$ can be used
![d6bfeeb8-a778-4669-94b9-7aa04ced3e7e.jpg](CSC4005 L1-2_files/d6bfeeb8-a778-4669-94b9-7aa04ced3e7e.jpg)
* broadcast, scatter, gather, and reduce operations (pvm_bcast(),pvm_scatter()...)use group of processes
* the PVM multicast operation, pvm_mcast(), is not a group operation
### MPI
* Only static process creation is supported in version 1. Use SPMD model of computation.
#### Communication
* communicators: is a *communication domain* that defines a set of processes that are allowed to communicate between themselves
* Types
    * intracommunicator: for communication within a group

    * intercommunicator: for communication between groups

* A process has unique rank in a group, and it could be a member of more than one group
#### Point-to-Point Communication
* Blocking Routines
    * A blocking send will send the message and return. This does not mean the message has been received, just the process is free to move on without adversely affecting the message
    * MPI_Send(buf, count, datatype, dest, tag, comm)

    * MPI_Recv(buf, count, datatype, src, tag, comm, status)

* Nonblocking Routines
    * send: will return "immediately" even before source location is safe to be altered. 

        * MPI_Isend (buf, count, datatype, dest, tag, comm, request)

    * receive: will return even if there is no message to accept
        * MPI_Irecv(buf, count, datatype, source, tag, comm, request)

    * Completion detected by MPI_Wait() and MPI_Test()

#### Send Communication Modes
![702c4e36-d951-43a6-98ad-8ee4454e2c19.jpg](CSC4005 L1-2_files/702c4e36-d951-43a6-98ad-8ee4454e2c19.jpg)
#### Collective Communication
* Involves set of processes, defined by an intra-communicator
### Time Complexity of MPI Program
#### Parallel Execution Time
* two parts: a computation part, and a communication part
* $t_p = t_{comp}+t_{comm}$
#### Communication Time
* $t_{comm} = t_{startup}+nt_{data}$
    * $ t_{startup}$ is startup time, sometimes called message latency, assumed constant

    * $t_{data}$ is transmission time to send data word, also assumed a constant

![af2df9f9-265f-4765-b76c-5890933e03bd.jpg](CSC4005 L1-2_files/af2df9f9-265f-4765-b76c-5890933e03bd.jpg)
![99f284eb-775a-4d6d-8001-2c60534daf6f.jpg](CSC4005 L1-2_files/99f284eb-775a-4d6d-8001-2c60534daf6f.jpg)
![b9a1e62d-b829-4302-b3de-d906800fdfb0.jpg](CSC4005 L1-2_files/b9a1e62d-b829-4302-b3de-d906800fdfb0.jpg)
#### Cost Optimal Algorithms
![209cd9c6-7246-4534-a521-58ae8befd6ab.jpg](CSC4005 L1-2_files/209cd9c6-7246-4534-a521-58ae8befd6ab.jpg)
### Time Complexity of Broadcast
#### on a Hypercube Network
![199fcd78-8344-4651-ab99-e722c1ed5bc4.jpg](CSC4005 L1-2_files/199fcd78-8344-4651-ab99-e722c1ed5bc4.jpg)


![eacdfdf4-9c01-455c-8b39-1c844196cd25.jpg](CSC4005 L1-2_files/eacdfdf4-9c01-455c-8b39-1c844196cd25.jpg)
#### on a Mesh Network
![f5af0a6a-bb2c-4e4e-a65b-f9e574fe4996.jpg](CSC4005 L1-2_files/f5af0a6a-bb2c-4e4e-a65b-f9e574fe4996.jpg)
#### on a Workstation Cluster
![a585f480-2f01-4865-b91f-79b0cf4a1961.jpg](CSC4005 L1-2_files/a585f480-2f01-4865-b91f-79b0cf4a1961.jpg)
#### Gather on a Hypercube Network
![8ed92a52-b657-498d-a960-c5788246cdb9.jpg](CSC4005 L1-2_files/8ed92a52-b657-498d-a960-c5788246cdb9.jpg)
